---
layout: default
title: Augustus Odena
---
<div class="blurb">
	<h1>Augustus Odena</h1>
	<img src="assets/purple.jpg" class="inline" width="20%" height="20%" align="right"
       hspace="50" vspace="1"/>
	<p>
    I'm an AI researcher and entrepreneur.
  </p>

	<p>
    I co-founded <a href="https://www.adept.ai">Adept</a>, where I was responsible for the research program. 
    We built the first <a href="https://www.adept.ai/blog/act-1">Computer Use Agents</a> and the <a href="https://www.adept.ai/blog/fuyu-8b">Fuyu</a> <a href="https://www.adept.ai/blog/adept-fuyu-heavy">family</a> of multi-modal models.
    I am now at Amazon, where I am now working on (IMO) the most important unsolved problem in AI research.
  </p>

	<p>
    Along with my Adept co-founder <a href="https://maxwellnye.com/">Max Nye</a>, I discovered/invented the notion of test-time-compute for Transformers.
    Our <a href="https://arxiv.org/abs/2112.00114">Scratchpad Technique</a> (popularly known as chain-of-thought) is the basis for all <a href="https://openai.com/o1/">modern</a> <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought">reasoning</a> <a href="https://ai.google.dev/gemini-api/docs/thinking">systems</a>.
    I also hold the <a href="https://patents.google.com/patent/US20230244938A1/en">patent</a> for Chain-of-Thought prompting.
  </p>

	<p>
    Prior to Adept, I was at Google Brain for ~5 years.
    In addition to the Scratchpad work, I did a lot of research on Program Synthesis.
    I led Brain's work on <a href="https://arxiv.org/abs/2108.07732">Program Synthesis with Large Language Models</a>
     and co-created Google Sheet's <a href="https://workspaceupdates.googleblog.com/2020/10/smart-fill-google-sheets-automate-data-entry.html">SmartFill Program Synthesizer</a>,
  </p>

	<p>
    Before I worked on program synthesis, I worked on generative modeling.
    Along with collaborators, I did a lot of the early work on scaling up 
    <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Networks</a>.
    That includes <a href="https://arxiv.org/abs/1610.09585">the first GAN to generate real ImageNet samples</a>,
    <a href="https://arxiv.org/abs/1805.08318">the paper that made Self-Attention work in GANs</a>, and 
    <a href="https://distill.pub/2016/deconv-checkerboard/">work that diagnosed and fixed the frequency artifacts that plagued early synthesis models</a>.
  </p>
    
	<p>
    I have also worked on <a href="https://arxiv.org/abs/1804.09170">Semi-Supervised Learning</a>
    and other, miscellaneous topics, such as 
    <a href="https://arxiv.org/abs/1807.10875">Coverage-Guided Fuzzing of Neural Networks</a>.
  </p>

	<p>
    I came to Brain by being in the first batch of <a href="https://ai.google/research/join-us/ai-residency/">Google Brain Residents</a>, 
    before which I worked at a startup called <a href="https://en.wikipedia.org/wiki/Nervana_Systems">Nervana Systems</a>.
    Before all of that, I was a trader: I worked at <a href="https://fiverings.com/">Five Rings Capital</a>
    and <a href="https://www.mlp.com/home/">Millennium</a>.
    Before I was a trader, I was a Math major at
    <a href="https://www.columbia.edu/">Columbia University</a>.
  </p>
</div><!-- /.blurb -->
